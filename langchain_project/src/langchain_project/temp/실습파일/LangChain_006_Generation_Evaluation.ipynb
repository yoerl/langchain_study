{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4019d1fe",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608326bb",
   "metadata": {},
   "source": [
    "`(1) LangSmith 설정 확인`\n",
    "- .env 파일에 아래 내용을 반영\n",
    "    - LANGCHAIN_TRACING_V2=true  \n",
    "    - LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"  \n",
    "    - LANGCHAIN_API_KEY=\"인증키를 입력하세요\"  \n",
    "    - LANGCHAIN_PROJECT=\"프로젝트명\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3cefd",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ace4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d6566",
   "metadata": {},
   "source": [
    "`(3) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7801daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langsmith 추척 여부:  false\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Langsmith tracing 여부를 확인 (true: langsmith 추척 활성화, false: langsmith 추척 비활성화)\n",
    "print(\"langsmith 추척 여부: \", os.getenv('LANGCHAIN_TRACING_V2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc341c8c",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a116eb7",
   "metadata": {},
   "source": [
    "`(1) Raw Documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4adad21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# 문서를 로드\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "\n",
    "final_docs = []\n",
    "\n",
    "with open('./data/final_docs_ver2.jsonl', 'rb') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        doc = Document(page_content=item['page_content'], metadata=item['metadata'])\n",
    "        final_docs.append(doc)\n",
    "\n",
    "print(len(final_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc73b2",
   "metadata": {},
   "source": [
    "`(2) Test Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b162940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...</td>\n",
       "      <td>['data/리비안_KR.txt']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>리비안의 초기 모델은 무엇인가요?</td>\n",
       "      <td>리비안의 초기 모델은 스포츠카 R1입니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...</td>\n",
       "      <td>['data/리비안_KR.txt']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>R1의 좌석 구성은 어떻게 되나요?</td>\n",
       "      <td>R1은 2+2 좌석 구성입니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...</td>\n",
       "      <td>['data/리비안_KR.txt']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>R1은 어떤 구조를 특징으로 하나요?</td>\n",
       "      <td>R1은 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전...</td>\n",
       "      <td>['data/테슬라_KR.txt']</td>\n",
       "      <td>['1']</td>\n",
       "      <td>테슬라는 어디에 본사를 두고 있나요?</td>\n",
       "      <td>테슬라는 텍사스주 오스틴에 본사를 두고 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전...</td>\n",
       "      <td>['data/테슬라_KR.txt']</td>\n",
       "      <td>['1']</td>\n",
       "      <td>테슬라는 언제 설립되었나요?</td>\n",
       "      <td>테슬라는 2003년에 설립되었습니다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context                   source  \\\n",
       "0  ['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...  ['data/리비안_KR.txt']   \n",
       "1  ['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...  ['data/리비안_KR.txt']   \n",
       "2  ['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...  ['data/리비안_KR.txt']   \n",
       "3  ['테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전...  ['data/테슬라_KR.txt']   \n",
       "4  ['테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전...  ['data/테슬라_KR.txt']   \n",
       "\n",
       "  doc_id              question                                        answer  \n",
       "0  ['0']    리비안의 초기 모델은 무엇인가요?                       리비안의 초기 모델은 스포츠카 R1입니다.  \n",
       "1  ['0']   R1의 좌석 구성은 어떻게 되나요?                             R1은 2+2 좌석 구성입니다.  \n",
       "2  ['0']  R1은 어떤 구조를 특징으로 하나요?  R1은 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 합니다.  \n",
       "3  ['1']  테슬라는 어디에 본사를 두고 있나요?                   테슬라는 텍사스주 오스틴에 본사를 두고 있습니다.  \n",
       "4  ['1']       테슬라는 언제 설립되었나요?                          테슬라는 2003년에 설립되었습니다.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 데이터셋에 대한 QA 생성 결과를 리뷰한 후 다시 로드\n",
    "import pandas as pd\n",
    "\n",
    "df_qa_test = pd.read_excel(\"./data/qa_test_revised.xlsx\")\n",
    "\n",
    "df_qa_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d2368",
   "metadata": {},
   "source": [
    "`(3) 검색도구 정의`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9a519",
   "metadata": {},
   "source": [
    "-  BM25 검색기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d9739e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: 테슬라의 회장은 누구인가요?\n",
      "검색 결과:\n",
      "- . 머스크는 최대 주주이자 회장으로서 회사를 현재의 성공으로 이끌었습니다. 회사 이름은 유명한 물리학자이자 전기공학자인 니콜라 테슬라의 이름을 따서 지어졌습니다. 테슬라는 2010년 6월 나스닥에 상장되었습니다\n",
      "\n",
      "(참고: 이 문서는 테슬라에 대한 정보를 담고 있습니다.) [출처: data/테슬라_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "- .\n",
      "\n",
      "2023년 테슬라는 1,808,581대의 차량을 판매하여 2022년에 비해 37.65% 증가했습니다. 2012년부터 2023년 3분기까지 테슬라의 전 세계 누적 판매량은 4,962,975대를 초과했습니다. SMT Packaging에 따르면, 2023년 테슬라의 판매량은 전 세계 전기차 시장의 약 12.9%를 차지했습니다\n",
      "\n",
      "(참고: 이 문서는 테슬라에 대한 정보를 담고 있습니다.) [출처: data/테슬라_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BM25 검색기를 사용하기 위한 준비\n",
    "from krag.tokenizers import KiwiTokenizer\n",
    "from krag.retrievers import KiWiBM25RetrieverWithScore\n",
    "\n",
    "kiwi_tokenizer = KiwiTokenizer(model_type='knlm', typos='basic')\n",
    "\n",
    "bm25_db = KiWiBM25RetrieverWithScore(\n",
    "        documents=final_docs, \n",
    "        kiwi_tokenizer=kiwi_tokenizer, \n",
    "        k=2, \n",
    "        threshold=0.0,\n",
    "    )\n",
    "\n",
    "# BM25 검색기를 사용하여 문서 검색\n",
    "query = \"테슬라의 회장은 누구인가요?\"\n",
    "retrieved_docs = bm25_db.invoke(query, 2)\n",
    "\n",
    "# 검색 결과 출력 \n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48202b63",
   "metadata": {},
   "source": [
    "- Vector Store 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb550c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: 테슬라의 회장은 누구인가요?\n",
      "검색 결과:\n",
      "- 테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전기차 제조업체입니다. 2003년 마틴 에버하드(CEO)와 마크 타페닝(CFO)에 의해 설립된 테슬라는 2004년 페이팔과 Zip2의 공동 창업자인 일론 머스크의 참여로 큰 전환점을 맞았습니다\n",
      "\n",
      "(참고: 이 문서는 테슬라에 대한 정보를 담고 있습니다.) [출처: data/테슬라_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "- . 머스크는 최대 주주이자 회장으로서 회사를 현재의 성공으로 이끌었습니다. 회사 이름은 유명한 물리학자이자 전기공학자인 니콜라 테슬라의 이름을 따서 지어졌습니다. 테슬라는 2010년 6월 나스닥에 상장되었습니다\n",
      "\n",
      "(참고: 이 문서는 테슬라에 대한 정보를 담고 있습니다.) [출처: data/테슬라_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 벡터스토어 로드\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    embedding_function=embeddings_model,\n",
    "    collection_name=\"hf_bge_m3\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "chroma_k = chroma_db.as_retriever(\n",
    "    search_kwargs={'k': 2},\n",
    ")\n",
    "\n",
    "# 벡터스토어를 사용하여 문서 검색\n",
    "query = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "retrieved_docs = chroma_k.invoke(query)\n",
    "\n",
    "# 검색 결과 출력\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83a642",
   "metadata": {},
   "source": [
    "- Emsemble Hybrid Search 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17908b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: 테슬라의 회장은 누구인가요?\n",
      "검색 결과:\n",
      "- . 머스크는 최대 주주이자 회장으로서 회사를 현재의 성공으로 이끌었습니다. 회사 이름은 유명한 물리학자이자 전기공학자인 니콜라 테슬라의 이름을 따서 지어졌습니다. 테슬라는 2010년 6월 나스닥에 상장되었습니다\n",
      "\n",
      "(참고: 이 문서는 테슬라에 대한 정보를 담고 있습니다.) [출처: data/테슬라_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "- 테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전기차 제조업체입니다. 2003년 마틴 에버하드(CEO)와 마크 타페닝(CFO)에 의해 설립된 테슬라는 2004년 페이팔과 Zip2의 공동 창업자인 일론 머스크의 참여로 큰 전환점을 맞았습니다\n",
      "\n",
      "(참고: 이 문서는 테슬라에 대한 정보를 담고 있습니다.) [출처: data/테슬라_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "- .\n",
      "\n",
      "2023년 테슬라는 1,808,581대의 차량을 판매하여 2022년에 비해 37.65% 증가했습니다. 2012년부터 2023년 3분기까지 테슬라의 전 세계 누적 판매량은 4,962,975대를 초과했습니다. SMT Packaging에 따르면, 2023년 테슬라의 판매량은 전 세계 전기차 시장의 약 12.9%를 차지했습니다\n",
      "\n",
      "(참고: 이 문서는 테슬라에 대한 정보를 담고 있습니다.) [출처: data/테슬라_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "- .\n",
      "\n",
      "리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페로 피터 스티븐스가 디자인했습니다. 이 차는 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 하며, 2013년 말에서 2014년 초 사이에 생산이 예상되었습니다\n",
      "\n",
      "(참고: 이 문서는 리비안에 대한 정보를 담고 있습니다.) [출처: data/리비안_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "- 리비안은 MIT 박사 출신 RJ 스카린지가 2009년에 설립한 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율 전기차에 집중한 리비안은 2015년 대규모 투자를 통해 크게 성장하며 미시간과 베이 지역에 연구소를 설립했습니다. 주요 공급업체와의 접근성을 높이기 위해 본사를 미시간주 리보니아로 이전했습니다\n",
      "\n",
      "(참고: 이 문서는 리비안에 대한 정보를 담고 있습니다.) [출처: data/리비안_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "\n",
    "# 검색기 초기화 \n",
    "def create_hybrid_retriever(bm25_db, vector_db, k: int = 4):\n",
    "\n",
    "    bm25_db.k = k\n",
    "    chroma_k = vector_db.as_retriever(search_kwargs={'k': k})\n",
    "\n",
    "    retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_db, chroma_k],\n",
    "        weights=[0.5, 0.5],\n",
    "    )\n",
    "\n",
    "    return retriever\n",
    "\n",
    "hybrid_retriever = create_hybrid_retriever(bm25_db, chroma_db, k=4)\n",
    "\n",
    "query = \"테슬라의 회장은 누구인가요?\"\n",
    "retrieved_docs = hybrid_retriever.invoke(query)\n",
    "\n",
    "# 검색 결과 출력\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de555f3",
   "metadata": {},
   "source": [
    "- Cross-Encoder 알고리즘에 기반하여 재정렬 (Re-rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "207ad64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: 테슬라 회장은 누구인가요?\n",
      "검색 결과:\n",
      "- 테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전기차 제조업체입니다. 2003년 마틴 에버하드(CEO)와 마크 타페닝(CFO)에 의해 설립된 테슬라는 2004년 페이팔과 Zip2의 공동 창업자인 일론 머스크의 참여로 큰 전환점을 맞았습니다\n",
      "\n",
      "(참고: 이 문서는 테슬라에 대한 정보를 담고 있습니다.) [출처: data/테슬라_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "- . 머스크는 최대 주주이자 회장으로서 회사를 현재의 성공으로 이끌었습니다. 회사 이름은 유명한 물리학자이자 전기공학자인 니콜라 테슬라의 이름을 따서 지어졌습니다. 테슬라는 2010년 6월 나스닥에 상장되었습니다\n",
      "\n",
      "(참고: 이 문서는 테슬라에 대한 정보를 담고 있습니다.) [출처: data/테슬라_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "- 리비안은 MIT 박사 출신 RJ 스카린지가 2009년에 설립한 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율 전기차에 집중한 리비안은 2015년 대규모 투자를 통해 크게 성장하며 미시간과 베이 지역에 연구소를 설립했습니다. 주요 공급업체와의 접근성을 높이기 위해 본사를 미시간주 리보니아로 이전했습니다\n",
      "\n",
      "(참고: 이 문서는 리비안에 대한 정보를 담고 있습니다.) [출처: data/리비안_KR.txt]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "re_ranker = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "cross_encoder_reranker_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=re_ranker, \n",
    "    base_retriever=hybrid_retriever,\n",
    ")\n",
    "\n",
    "question = \"테슬라 회장은 누구인가요?\"\n",
    "\n",
    "retrieved_docs = cross_encoder_reranker_retriever.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5781f4",
   "metadata": {},
   "source": [
    "## 3. LLM 유형 및 답변 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f1066",
   "metadata": {},
   "source": [
    "### 3-1 RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b94247f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 쿼리에 대한 검색 결과를 한꺼번에 Context로 전달해서 답변을 생성\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def create_rag_chain(retriever, llm):\n",
    "\n",
    "    template = \"\"\"Answer the following question based on this context. If the context is not relevant to the question, just answer with '답변에 필요한 근거를 찾지 못했습니다.'\n",
    "\n",
    "    [Context]\n",
    "    {context}\n",
    "\n",
    "    [Question]\n",
    "    {question}\n",
    "\n",
    "    [Answer]\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([f\"{doc.page_content}\" for doc in docs])\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} \n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39e7552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: 테슬라의 회장은 누구인가요?\n",
      "답변:\n",
      "일론 머스크입니다.\n"
     ]
    }
   ],
   "source": [
    "# RAG 체인 생성 및 테스트\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "openai_rag_chain = create_rag_chain(cross_encoder_reranker_retriever, llm)\n",
    "\n",
    "question = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "answer = openai_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"답변:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc0afb",
   "metadata": {},
   "source": [
    "### 3-2 주요 모델 공급자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539e9c1",
   "metadata": {},
   "source": [
    "`(1) Anthropic Claude API`\n",
    "\n",
    "https://www.anthropic.com/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eb5fbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: 테슬라의 회장은 누구인가요?\n",
      "답변:\n",
      "문서에 따르면 테슬라의 회장은 일론 머스크입니다. 문서에서 \"머스크는 최대 주주이자 회장으로서 회사를 현재의 성공으로 이끌었습니다.\"라고 명시되어 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# ChatAnthropic - LLM 모델 \n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# 모델 로드 \n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    temperature=0,\n",
    "    max_tokens=200, \n",
    ")\n",
    "\n",
    "# RAG 체인 생성 및 테스트\n",
    "anthropic_rag_chain = create_rag_chain(cross_encoder_reranker_retriever, llm)\n",
    "\n",
    "question = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "answer = anthropic_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"답변:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7045556",
   "metadata": {},
   "source": [
    "`(2) Google Gemini API`\n",
    "\n",
    "https://ai.google.dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7564cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: 테슬라의 회장은 누구인가요?\n",
      "답변:\n",
      "일론 머스크입니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ChatGoogleGenerativeAI - LLM 모델 \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 모델 로드 \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# RAG 체인 생성 및 테스트\n",
    "google_genai_rag_chain = create_rag_chain(cross_encoder_reranker_retriever, llm)\n",
    "\n",
    "question = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "answer = google_genai_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"답변:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988de355",
   "metadata": {},
   "source": [
    "`(3) Ollama - 오픈소스 LLM`\n",
    "\n",
    "https://ollama.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d20b039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: 테슬라의 회장은 누구인가요?\n",
      "답변:\n",
      "일론 머스크입니다.\n"
     ]
    }
   ],
   "source": [
    "# ChatOllama - LLM 모델 \n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 모델 로드 \n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.1\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 100,\n",
    ")\n",
    "\n",
    "# RAG 체인 생성 및 테스트\n",
    "ollama_rag_chain = create_rag_chain(cross_encoder_reranker_retriever, llm)\n",
    "\n",
    "question = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "answer = ollama_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"답변:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e78dd2",
   "metadata": {},
   "source": [
    "`(4) Groq API - 오픈소스 LLM`\n",
    "\n",
    "https://groq.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75a2e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리: 테슬라의 회장은 누구인가요?\n",
      "답변:\n",
      "일론 머스크\n"
     ]
    }
   ],
   "source": [
    "# ChatGroq - LLM 모델 \n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# 모델 로드 \n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "# RAG 체인 생성 및 테스트\n",
    "groq_rag_chain = create_rag_chain(cross_encoder_reranker_retriever, llm)\n",
    "\n",
    "question = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "answer = groq_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"답변:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04483861",
   "metadata": {},
   "source": [
    "## 4. RAG 답변 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb75c5",
   "metadata": {},
   "source": [
    "### 4-1. Metric Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f7f18",
   "metadata": {},
   "source": [
    "`(1) Embedding Distance`\n",
    "- 임베딩 거리를 사용하여 예측 문자열과 참조 레이블 문자열 간의 의미적 유사성을 측정\n",
    "- 계산된 거리 점수가 낮을수록 두 문자열의 의미가 더 유사함을 나타내며, 이 방법은 단순 문자열 비교보다 더 풍부한 의미적 평가가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a1ac065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "의미가 다른 문장 비교 결과: {'score': 0.101075617379074}\n",
      "의미가 비슷한 문장 비교 결과: {'score': 0.02741902364464399}\n"
     ]
    }
   ],
   "source": [
    "# LangChain EmbeddingDistanceEvalChain 활용 \n",
    "\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType, EmbeddingDistance\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Evaluator 초기화\n",
    "embedding_evaluator = load_evaluator(\n",
    "    evaluator=EvaluatorType.EMBEDDING_DISTANCE,      # 임베딩 거리를 기반으로 평가\n",
    "    distance_metric=EmbeddingDistance.COSINE,        # 코사인 유사도 사용\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0), # OpenAI LLM 사용\n",
    "    )\n",
    "\n",
    "\n",
    "# 의미가 다른 문장 비교\n",
    "result1 = embedding_evaluator.evaluate_strings(prediction=\"나는 학교에 갈 것이다\", reference=\"나는 집에 있을 것이다\")\n",
    "print(\"의미가 다른 문장 비교 결과:\", result1)\n",
    "\n",
    "# 의미가 비슷한 문장 비교\n",
    "result2 = embedding_evaluator.evaluate_strings(prediction=\"나는 학교에 갈 것이다\", reference=\"나는 학교로 향할 것이다\")\n",
    "print(\"의미가 비슷한 문장 비교 결과:\", result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14549cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...</td>\n",
       "      <td>['data/리비안_KR.txt']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>리비안의 초기 모델은 무엇인가요?</td>\n",
       "      <td>리비안의 초기 모델은 스포츠카 R1입니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...</td>\n",
       "      <td>['data/리비안_KR.txt']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>R1의 좌석 구성은 어떻게 되나요?</td>\n",
       "      <td>R1은 2+2 좌석 구성입니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...</td>\n",
       "      <td>['data/리비안_KR.txt']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>R1은 어떤 구조를 특징으로 하나요?</td>\n",
       "      <td>R1은 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전...</td>\n",
       "      <td>['data/테슬라_KR.txt']</td>\n",
       "      <td>['1']</td>\n",
       "      <td>테슬라는 어디에 본사를 두고 있나요?</td>\n",
       "      <td>테슬라는 텍사스주 오스틴에 본사를 두고 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전...</td>\n",
       "      <td>['data/테슬라_KR.txt']</td>\n",
       "      <td>['1']</td>\n",
       "      <td>테슬라는 언제 설립되었나요?</td>\n",
       "      <td>테슬라는 2003년에 설립되었습니다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context                   source  \\\n",
       "0  ['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...  ['data/리비안_KR.txt']   \n",
       "1  ['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...  ['data/리비안_KR.txt']   \n",
       "2  ['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2...  ['data/리비안_KR.txt']   \n",
       "3  ['테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전...  ['data/테슬라_KR.txt']   \n",
       "4  ['테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전...  ['data/테슬라_KR.txt']   \n",
       "\n",
       "  doc_id              question                                        answer  \n",
       "0  ['0']    리비안의 초기 모델은 무엇인가요?                       리비안의 초기 모델은 스포츠카 R1입니다.  \n",
       "1  ['0']   R1의 좌석 구성은 어떻게 되나요?                             R1은 2+2 좌석 구성입니다.  \n",
       "2  ['0']  R1은 어떤 구조를 특징으로 하나요?  R1은 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 합니다.  \n",
       "3  ['1']  테슬라는 어디에 본사를 두고 있나요?                   테슬라는 텍사스주 오스틴에 본사를 두고 있습니다.  \n",
       "4  ['1']       테슬라는 언제 설립되었나요?                          테슬라는 2003년에 설립되었습니다.  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 데이터셋에 대한 평가\n",
    "df_qa_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb96601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 리비안의 초기 모델은 무엇인가요?\n",
      "Ground Truth: 리비안의 초기 모델은 스포츠카 R1입니다.\n",
      "Prediction: 리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)입니다.\n",
      "Distance Score: {'score': 0.03528604311768424}\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 샘플에 대해 평가 - ground truth와 비교\n",
    "\n",
    "question = df_qa_test.iloc[0]['question']\n",
    "ground_truth = df_qa_test.iloc[0]['answer']\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "\n",
    "# OpenAI LLM을 사용하여 예측 생성\n",
    "openai_prediction = openai_rag_chain.invoke(question)\n",
    "print(\"Prediction:\", openai_prediction)\n",
    "\n",
    "distance_score = embedding_evaluator.evaluate_strings(prediction=openai_prediction, reference=ground_truth)\n",
    "print(\"Distance Score:\", distance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef27968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페입니다.\n",
      "Distance Score: {'score': 0.05677878923512858}\n"
     ]
    }
   ],
   "source": [
    "### 다른 모델들에 대해 평가\n",
    "# Anthropiic LLM을 사용하여 예측 생성\n",
    "anthropic_prediction = anthropic_rag_chain.invoke(question)\n",
    "print(\"Prediction:\", anthropic_prediction)\n",
    "\n",
    "distance_score = embedding_evaluator.evaluate_strings(prediction=anthropic_prediction, reference=ground_truth)\n",
    "print(\"Distance Score:\", distance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e464eeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)입니다. \n",
      "\n",
      "Distance Score: {'score': 0.04182225632635739}\n"
     ]
    }
   ],
   "source": [
    "# Google Generative AI LLM을 사용하여 예측 생성\n",
    "google_genai_prediction = google_genai_rag_chain.invoke(question)\n",
    "print(\"Prediction:\", google_genai_prediction)\n",
    "\n",
    "distance_score = embedding_evaluator.evaluate_strings(prediction=google_genai_prediction, reference=ground_truth)\n",
    "print(\"Distance Score:\", distance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c38d0726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 스포츠카 R1(원래 이름은 Avera)입니다.\n",
      "Distance Score: {'score': 0.09345460934189409}\n"
     ]
    }
   ],
   "source": [
    "# Ollama LLM을 사용하여 예측 생성\n",
    "ollama_prediction = ollama_rag_chain.invoke(question)\n",
    "print(\"Prediction:\", ollama_prediction)\n",
    "\n",
    "distance_score = embedding_evaluator.evaluate_strings(prediction=ollama_prediction, reference=ground_truth)\n",
    "print(\"Distance Score:\", distance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3b201a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페입니다.\n",
      "Distance Score: {'score': 0.05677878923512858}\n"
     ]
    }
   ],
   "source": [
    "# Groq LLM을 사용하여 예측 생성\n",
    "groq_prediction = groq_rag_chain.invoke(question)\n",
    "print(\"Prediction:\", groq_prediction)\n",
    "\n",
    "distance_score = embedding_evaluator.evaluate_strings(prediction=groq_prediction, reference=ground_truth)\n",
    "print(\"Distance Score:\", distance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd2dc8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Anthropic': '0.057',\n",
      " 'Google': '0.053',\n",
      " 'Groq': '0.057',\n",
      " 'Ollama': '0.254',\n",
      " 'OpenAI': '0.035'}\n"
     ]
    }
   ],
   "source": [
    "# 모든 모델에 대해 평가 결과를 비교\n",
    "\n",
    "def embedding_evaluate_all_models(question, ground_truth):\n",
    "    results = {}\n",
    "    for model_name, rag_chain in {\n",
    "        \"OpenAI\": openai_rag_chain,\n",
    "        \"Anthropic\": anthropic_rag_chain,\n",
    "        \"Google\": google_genai_rag_chain,\n",
    "        \"Ollama\": ollama_rag_chain,\n",
    "        \"Groq\": groq_rag_chain,\n",
    "    }.items():\n",
    "        prediction = rag_chain.invoke(question)\n",
    "        distance_score = embedding_evaluator.evaluate_strings(prediction=prediction, reference=ground_truth)\n",
    "        results[model_name] = f\"{distance_score['score']:.3f}\"\n",
    "    return results\n",
    "\n",
    "question = df_qa_test.iloc[0]['question']\n",
    "ground_truth = df_qa_test.iloc[0]['answer']\n",
    "\n",
    "results = embedding_evaluate_all_models(question, ground_truth)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fa7b714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OpenAI</th>\n",
       "      <th>Anthropic</th>\n",
       "      <th>Google</th>\n",
       "      <th>Ollama</th>\n",
       "      <th>Groq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OpenAI Anthropic Google Ollama   Groq\n",
       "0   0.035     0.057  0.055  0.119  0.057\n",
       "1   0.004     0.092  0.057  0.067  0.012\n",
       "2  -0.000     0.016  0.025  0.128  0.345"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 데이터셋에 대해 A/B 테스트 - 여기서는 3개만 평가 (데이터프레임으로 정리)\n",
    "\n",
    "def embedding_evaluate_qa_dataset(df_qa_test):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(3):\n",
    "        question = df_qa_test.iloc[i]['question']\n",
    "        ground_truth = df_qa_test.iloc[i]['answer']\n",
    "        result = embedding_evaluate_all_models(question, ground_truth)\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_result_embedding = embedding_evaluate_qa_dataset(df_qa_test.iloc[:3])\n",
    "df_result_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc1df79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI       0.013000\n",
       "Google       0.045667\n",
       "Anthropic    0.055000\n",
       "Ollama       0.104667\n",
       "Groq         0.138000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding Distance가 낮을 수록 좋은 결과\n",
    "df_result_embedding.astype(float).mean().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f520bf",
   "metadata": {},
   "source": [
    "`(2) Cross-Encoder 활용`\n",
    "- 두 문장을 동시에 인코딩하여 직접적으로 유사성을 평가 (개별 문장을 따로 인코딩하는 일반적인 Bi-Encoder 임베딩 방식과 다름)\n",
    "- 크로스 인코더는 두 문장의 상호작용을 직접 모델링하므로, 일반적으로 더 정확한 유사성 점수를 제공\n",
    "- 이 방법은 의미적 유사성을 더 정확하게 측정할 수 있지만, 계산 비용이 더 높다는 점을 유의해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d83027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def calculate_cross_encoder_similarity(\n",
    "        query: str, \n",
    "        prediction: str, \n",
    "        model_name: str = \"BAAI/bge-reranker-v2-m3\",\n",
    "        ) -> float:\n",
    "    \"\"\"\n",
    "    주어진 query와 prediction 사이의 의미적 유사성을 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    query (str): 기준이 되는 쿼리 문장\n",
    "    prediction (str): 유사성을 비교할 예측 문장\n",
    "    model_name (str): 사용할 크로스 인코더 모델 이름 (기본값: \"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "    Returns:\n",
    "    float: 두 문장 간의 유사성 점수\n",
    "    \"\"\"\n",
    "    # 크로스 인코더 모델을 불러옵니다.\n",
    "    cross_encoder_model = CrossEncoder(model_name)\n",
    "\n",
    "    # 크로스 인코더를 사용하여 유사성 점수를 계산합니다.\n",
    "    sentence_pairs = [[query, prediction]]\n",
    "    similarity_scores = cross_encoder_model.predict(sentence_pairs)\n",
    "\n",
    "    return similarity_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5be6ae16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 리비안의 초기 모델은 무엇인가요?\n",
      "Ground Truth: 리비안의 초기 모델은 스포츠카 R1입니다.\n",
      "Prediction: 리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)입니다.\n",
      "유사성 점수: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 샘플에 대해 유사성 점수 계산 - 점수가 높을수록 더 유사함\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "print(\"Prediction:\", openai_prediction)\n",
    "\n",
    "similarity = calculate_cross_encoder_similarity(ground_truth, openai_prediction)\n",
    "print(f\"유사성 점수: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3404f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI\n",
      "리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)입니다.\n",
      "\n",
      "Anthropic\n",
      "리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페입니다.\n",
      "\n",
      "Google\n",
      "리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)입니다. \n",
      "\n",
      "\n",
      "Ollama\n",
      "스포츠카 R1(원래 이름은 Avera)입니다. 이 차는 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 하며, 2013년 말에서 2014년 초 사이에 생산이 예상되었습니다.\n",
      "\n",
      "Groq\n",
      "\n",
      "\n",
      "OpenAI\n",
      "R1의 좌석 구성은 2+2 좌석입니다.\n",
      "\n",
      "Anthropic\n",
      "문맥에 따르면 리비안의 초기 모델인 R1은 2+2 좌석의 미드 엔진 하이브리드 쿠페라고 되어 있습니다. 따라서 R1의 좌석 구성은 2+2 좌석이라고 답변할 수 있습니다.\n",
      "\n",
      "Google\n",
      "R1은 2+2 좌석의 미드 엔진 하이브리드 쿠페입니다. \n",
      "\n",
      "\n",
      "Ollama\n",
      "2+2 좌석입니다.\n",
      "\n",
      "Groq\n",
      "R1의 좌석 구성은 2+2 좌석입니다.\n",
      "\n",
      "OpenAI\n",
      "R1은 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 합니다.\n",
      "\n",
      "Anthropic\n",
      "문맥에 따르면, R1은 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 합니다.\n",
      "\n",
      "Google\n",
      "R1은 **모듈식 캡슐 구조**를 특징으로 합니다. 이 구조는 쉽게 교체 가능한 본체 패널을 갖추고 있습니다. \n",
      "\n",
      "\n",
      "Ollama\n",
      "모듈식 캡슐 구조입니다.\n",
      "\n",
      "Groq\n",
      "R1은 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 합니다.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OpenAI</th>\n",
       "      <th>Anthropic</th>\n",
       "      <th>Google</th>\n",
       "      <th>Ollama</th>\n",
       "      <th>Groq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  OpenAI Anthropic Google Ollama   Groq\n",
       "0  1.000     1.000  1.000  0.474  0.000\n",
       "1  1.000     0.999  0.974  0.998  1.000\n",
       "2  1.000     1.000  1.000  0.996  1.000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 모델에 대해 평가 결과를 비교\n",
    "\n",
    "def cross_encoder_evaluate_all_models(question, ground_truth):\n",
    "    results = {}\n",
    "    for model_name, rag_chain in {\n",
    "        \"OpenAI\": openai_rag_chain,\n",
    "        \"Anthropic\": anthropic_rag_chain,\n",
    "        \"Google\": google_genai_rag_chain,\n",
    "        \"Ollama\": ollama_rag_chain,\n",
    "        \"Groq\": groq_rag_chain,\n",
    "    }.items():\n",
    "        prediction = rag_chain.invoke(question)\n",
    "        print(model_name)\n",
    "        print(prediction)\n",
    "        print()\n",
    "        similarity = calculate_cross_encoder_similarity(ground_truth, prediction)\n",
    "        results[model_name] = f\"{similarity:.3f}\"\n",
    "    return results\n",
    "\n",
    "\n",
    "# 전체 데이터셋에 대해 A/B 테스트 - 여기서는 3개만 평가 (데이터프레임으로 정리)\n",
    "\n",
    "def cross_encoder_evaluate_qa_dataset(df_qa_test):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(3):\n",
    "        question = df_qa_test.iloc[i]['question']\n",
    "        ground_truth = df_qa_test.iloc[i]['answer']\n",
    "        result = cross_encoder_evaluate_all_models(question, ground_truth)\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "df_result_cross_encoder = cross_encoder_evaluate_qa_dataset(df_qa_test.iloc[:3])\n",
    "df_result_cross_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f292ddee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI       1.000000\n",
       "Anthropic    0.999667\n",
       "Google       0.991333\n",
       "Ollama       0.822667\n",
       "Groq         0.666667\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Encoder Similarity가 높을 수록 좋은 결과\n",
    "df_result_cross_encoder.astype(float).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b24443",
   "metadata": {},
   "source": [
    "`(3) Rouge Metric`\n",
    "- ROUGE 메트릭은 두 문장의 유사도를 평가하는 데 널리 사용되는 방법으로, 특히 텍스트 요약과 기계 번역 분야에서 유용함\n",
    "- 단어 중첩을 기반으로 하여 계산이 빠르고 해석이 쉽다는 장점이 있지만, 깊은 의미적 유사성을 포착하는 데는 한계가 있음\n",
    "- 사용 목적과 컨텍스트에 따라 적절히 선택하거나 다른 메트릭과 조합하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38708625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from korouge_score import rouge_scorer\n",
    "from typing import List, Dict\n",
    "\n",
    "def calculate_rouge_similarity(\n",
    "        query: str, \n",
    "        prediction: str, \n",
    "        rouge_types: List[str] = ['rouge1', 'rouge2', 'rougeL'],\n",
    "        ) -> Dict[str, float]:\n",
    "    \n",
    "    \"\"\"\n",
    "    주어진 쿼리 문장과 예측 문장 사이의 선택된 ROUGE 점수를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    query (str): 기준이 되는 쿼리 문장\n",
    "    prediction (str): 유사성을 비교할 예측 문장\n",
    "    rouge_types (List[str]): 계산할 ROUGE 메트릭 리스트 (기본값: ['rouge1', 'rouge2', 'rougeL'])\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, float]: 선택된 ROUGE 메트릭의 F1 점수를 포함하는 딕셔너리\n",
    "    \"\"\"\n",
    "    # 입력된 ROUGE 유형의 유효성을 검사합니다.\n",
    "    valid_rouge_types = set(['rouge1', 'rouge2', 'rougeL'])\n",
    "    rouge_types = [rt for rt in rouge_types if rt in valid_rouge_types]\n",
    "    \n",
    "    if not rouge_types:\n",
    "        raise ValueError(\"유효한 ROUGE 유형이 제공되지 않았습니다.\")\n",
    "\n",
    "    # ROUGE scorer 객체를 초기화합니다.\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types, use_stemmer=True)\n",
    "\n",
    "    # ROUGE 점수를 계산합니다.\n",
    "    scores = scorer.score(query, prediction)\n",
    "\n",
    "    # 결과를 정리합니다.\n",
    "    result = {rouge_type: scores[rouge_type].fmeasure for rouge_type in rouge_types}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f30f525d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 리비안의 초기 모델은 무엇인가요?\n",
      "Ground Truth: 리비안의 초기 모델은 스포츠카 R1입니다.\n",
      "Prediction: 리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)입니다.\n",
      "Rouge 점수: 0.6667\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 샘플에 대해 유사성 점수 계산 - 점수가 높을수록 더 유사함\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "print(\"Prediction:\", openai_prediction)\n",
    "\n",
    "rouge_scores = calculate_rouge_similarity(ground_truth, openai_prediction, ['rouge1'])\n",
    "print(f\"Rouge 점수: {rouge_scores['rouge1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b523331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OpenAI</th>\n",
       "      <th>Anthropic</th>\n",
       "      <th>Google</th>\n",
       "      <th>Ollama</th>\n",
       "      <th>Groq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.609</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  OpenAI Anthropic Google Ollama   Groq\n",
       "0  0.600     0.375  0.545  0.000  0.375\n",
       "1  1.000     0.080  0.000  0.000  0.000\n",
       "2  1.000     0.917  0.417  0.609  1.000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 모델에 대해 평가 결과를 비교\n",
    "\n",
    "def rouge_evaluate_all_models(question, ground_truth):\n",
    "    results = {}\n",
    "    for model_name, rag_chain in {\n",
    "        \"OpenAI\": openai_rag_chain,\n",
    "        \"Anthropic\": anthropic_rag_chain,\n",
    "        \"Google\": google_genai_rag_chain,\n",
    "        \"Ollama\": ollama_rag_chain,\n",
    "        \"Groq\": groq_rag_chain,\n",
    "    }.items():\n",
    "        prediction = rag_chain.invoke(question)\n",
    "        rouge_scores = calculate_rouge_similarity(ground_truth, prediction, ['rouge2'])\n",
    "        results[model_name] = f\"{rouge_scores['rouge2']:.3f}\"\n",
    "    return results\n",
    "\n",
    "# 전체 데이터셋에 대해 A/B 테스트 - 여기서는 3개만 평가 (데이터프레임으로 정리)\n",
    "\n",
    "def rouge_evaluate_qa_dataset(df_qa_test):\n",
    "    \n",
    "        results = []\n",
    "    \n",
    "        for i in range(3):\n",
    "            question = df_qa_test.iloc[i]['question']\n",
    "            ground_truth = df_qa_test.iloc[i]['answer']\n",
    "            result = rouge_evaluate_all_models(question, ground_truth)\n",
    "            results.append(result)\n",
    "    \n",
    "        return pd.DataFrame(results)    \n",
    "\n",
    "df_result_rouge = rouge_evaluate_qa_dataset(df_qa_test.iloc[:3])\n",
    "df_result_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd98ef2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI       0.866667\n",
       "Groq         0.458333\n",
       "Anthropic    0.457333\n",
       "Google       0.320667\n",
       "Ollama       0.203000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROUGE 점수가 높을 수록 좋은 결과\n",
    "df_result_rouge.astype(float).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b56bb2",
   "metadata": {},
   "source": [
    "### 4-2. LLM-as-judge\n",
    "- LLM은 인간과 유사한 판단을 제공할 수 있어, 단순한 단어 중첩 기반 메트릭보다 더 깊은 의미적 관련성을 포착할 수 있음\n",
    "- ROUGE와 같은 전통적인 메트릭보다 더 정교한 평가를 제공할 수 있지만, LLM의 판단에 의존하기 때문에 완전한 객관성을 보장하기는 어려움\n",
    "- 또한 API 사용에 따른 비용과 처리 시간이 필요하다는 점을 고려해야 함\n",
    "- 따라서 여러 평가 방법을 조합하여 사용 필요 (예를 들어, ROUGE 점수로 빠른 초기 스크리닝을 하고, 중요한 케이스에 대해서만 이 LLM 기반 평가를 수행하는 방식을 고려)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63659d6c",
   "metadata": {},
   "source": [
    "`(1) QA Evaluation - 사용자 질문에 대한 정확성, 관련성을 평가 (Y/N, 0/1)`\n",
    "\n",
    "1. \"qa\" 평가기: 사용자 질문에 대한 응답의 정확성을 직접적으로 평가\n",
    "\n",
    "2. \"context_qa\" 평가기: 더 넓은 맥락을 고려하여 응답의 정확성을 평가\n",
    "\n",
    "3. \"cot_qa\" 평가기: CoT(Chain of Thought) 추론을 통해 더 심층적인 평가를 수행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cde0a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: R1의 좌석 구성은 어떻게 되나요?\n",
      "Context: ['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페로 피터 스티븐스가 디자인했습니다. 이 차는 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 하며, 2013년 말에서 2014년 초 사이에 생산이 예상되었습니다\\n\\n(참고: 이 문서는 리비안에 대한 정보를 담고 있습니다.)']\n",
      "Ground Truth: R1은 2+2 좌석 구성입니다.\n",
      "Prediction: R1의 좌석 구성은 2+2 좌석입니다.\n"
     ]
    }
   ],
   "source": [
    "# 2번째 샘플에 대해 예측 수행\n",
    "question = df_qa_test.iloc[1]['question']\n",
    "context = df_qa_test.iloc[1]['context']\n",
    "ground_truth = df_qa_test.iloc[1]['answer']\n",
    "groq_prediction = groq_rag_chain.invoke(question)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Context:\", context)\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "print(\"Prediction:\", groq_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bf9912e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'GRADE: CORRECT', 'value': 'CORRECT', 'score': 1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# QA Evaluator 초기화\n",
    "qa_evaluator = load_evaluator(\n",
    "    evaluator=\"qa\",  \n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0), # OpenAI LLM 사용\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "qa_eval_result = qa_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=ground_truth,       # 평가 기준: 정답\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "qa_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2665a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'GRADE: CORRECT', 'value': 'CORRECT', 'score': 1}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Context QA Evaluator 초기화\n",
    "context_qa_evaluator = load_evaluator(\n",
    "    evaluator=\"context_qa\",  \n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0), # OpenAI LLM 사용\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "context_qa_eval_result = context_qa_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=context,            # 평가 기준: 문맥\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "context_qa_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ca74379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'To evaluate the student\\'s answer, I will follow these steps:\\n\\n1. **Identify the Question**: The question asks about the seating configuration of the R1 model.\\n\\n2. **Review the Context**: The context provided states that the R1 model is a 2+2 seating mid-engine hybrid coupe. This indicates that the seating arrangement consists of two front seats and two rear seats.\\n\\n3. **Analyze the Student\\'s Answer**: The student states that \"R1의 좌석 구성은 2+2 좌석입니다,\" which translates to \"The seating configuration of the R1 is 2+2 seats.\" \\n\\n4. **Compare the Student\\'s Answer to the Context**: The student\\'s answer directly matches the information provided in the context. The context confirms that the R1 has a 2+2 seating arrangement, which is exactly what the student has stated.\\n\\n5. **Determine Factual Accuracy**: Since the student\\'s answer accurately reflects the information given in the context without any conflicting statements, it is factually correct.\\n\\nBased on this reasoning, I conclude that the student\\'s answer is correct.\\n\\nGRADE: CORRECT',\n",
       " 'value': 'CORRECT',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COT QA Evaluator 초기화\n",
    "cot_qa_evaluator = load_evaluator(\n",
    "    evaluator=\"cot_qa\",  \n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0), # OpenAI LLM 사용\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "cot_qa_eval_result = cot_qa_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=context,            # 평가 기준: 문맥\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "cot_qa_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e25d6732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To evaluate the student's answer, I will follow these steps:\n",
      "\n",
      "1. **Identify the Question**: The question asks about the seating configuration of the R1 model.\n",
      "\n",
      "2. **Review the Context**: The context provided states that the R1 model is a 2+2 seating mid-engine hybrid coupe. This indicates that the seating arrangement consists of two front seats and two rear seats.\n",
      "\n",
      "3. **Analyze the Student's Answer**: The student states that \"R1의 좌석 구성은 2+2 좌석입니다,\" which translates to \"The seating configuration of the R1 is 2+2 seats.\" \n",
      "\n",
      "4. **Compare the Student's Answer to the Context**: The student's answer directly matches the information provided in the context. The context confirms that the R1 has a 2+2 seating arrangement, which is exactly what the student has stated.\n",
      "\n",
      "5. **Determine Factual Accuracy**: Since the student's answer accurately reflects the information given in the context without any conflicting statements, it is factually correct.\n",
      "\n",
      "Based on this reasoning, I conclude that the student's answer is correct.\n",
      "\n",
      "GRADE: CORRECT\n"
     ]
    }
   ],
   "source": [
    "print(cot_qa_eval_result[\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9705e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'QUESTION: R1의 좌석 구성은 어떻게 되나요?  \\nCONTEXT: [\\'.\\\\n\\\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페로 피터 스티븐스가 디자인했습니다. 이 차는 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 하며, 2013년 말에서 2014년 초 사이에 생산이 예상되었습니다\\\\n\\\\n(참고: 이 문서는 리비안에 대한 정보를 담고 있습니다.)\\']  \\nSYSTEM ANSWER: R1의 좌석 구성은 2+2 좌석입니다.  \\n\\nEVALUATION:  \\n1. Factual Accuracy:  \\n   - 평가: 시스템의 답변은 \"R1의 좌석 구성은 2+2 좌석입니다.\"라고 명확하게 언급하고 있으며, 이는 제공된 문맥에서 \"2+2 좌석의 미드 엔진 하이브리드 쿠페\"라는 정보와 일치합니다.  \\n   - 설명: 따라서, 사실적으로 정확합니다.\\n\\n2. Relevance:  \\n   - 평가: 질문은 R1의 좌석 구성에 대한 것이며, 시스템의 답변은 이 질문에 직접적으로 답하고 있습니다.  \\n   - 설명: 답변은 질문의 핵심을 잘 반영하고 있어 관련성이 높습니다.\\n\\n3. Completeness:  \\n   - 평가: 시스템의 답변은 R1의 좌석 구성에 대한 질문에 대해 필요한 정보를 제공하고 있습니다.  \\n   - 설명: 그러나, \"2+2 좌석\"이라는 정보 외에 추가적인 설명이나 맥락이 없으므로, 완전성 측면에서 약간의 부족함이 있습니다. 예를 들어, 좌석 구성의 의미나 장점에 대한 설명이 포함되면 더 좋았을 것입니다.\\n\\n4. Conciseness:  \\n   - 평가: 시스템의 답변은 간결하고 핵심 정보를 잘 전달하고 있습니다.  \\n   - 설명: 불필요한 세부사항 없이 질문에 대한 직접적인 답변을 제공하고 있어 간결성 측면에서 우수합니다.\\n\\nOverall Assessment:  \\n시스템의 답변은 사실적으로 정확하고 관련성이 높으며, 간결하게 질문에 답변하고 있습니다. 그러나 완전성 측면에서 약간의 부족함이 있어, 추가적인 설명이 포함되면 더 좋았을 것입니다. 따라서, 이 답변은 일부 기준을 충족하지만 완전성에서 약간의 부족함이 있어 PARTIALLY CORRECT로 평가합니다.\\n\\nGRADE: PARTIALLY CORRECT',\n",
       " 'value': 'CORRECT',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COT QA Evaluator 초기화 (사용자 정의 프롬프트 사용)\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "CUSTOM_COT_QA_PROMPT = \"\"\"\n",
    "You are an expert evaluating the performance of a RAG (Retrieval-Augmented Generation) system. Your task is to assess the quality of the system's answer based on the given question, retrieved context, and the generated answer. Grade the answer as CORRECT, PARTIALLY CORRECT, or INCORRECT, and provide a detailed explanation. Please describe your evaluation process step by step to clearly show how you reached your conclusion.\n",
    "\n",
    "Use the following criteria for evaluation:\n",
    "1. Factual Accuracy: Does the answer align with the information in the retrieved context?\n",
    "2. Relevance: Does the answer appropriately address the question?\n",
    "3. Completeness: Does the answer cover all aspects of the question?\n",
    "4. Conciseness: Does the answer convey the key information without unnecessary details?\n",
    "\n",
    "Grading Criteria:\n",
    "- CORRECT: Meets all criteria with no errors\n",
    "- PARTIALLY CORRECT: Meets some criteria but has minor errors or omissions\n",
    "- INCORRECT: Contains major factual errors or is irrelevant to the question\n",
    "\n",
    "Evaluation Format:\n",
    "QUESTION: [The question content]\n",
    "CONTEXT: [The retrieved context]\n",
    "SYSTEM ANSWER: [The answer generated by the RAG system]\n",
    "EVALUATION:\n",
    "1. Factual Accuracy: [Assessment and explanation]\n",
    "2. Relevance: [Assessment and explanation]\n",
    "3. Completeness: [Assessment and explanation]\n",
    "4. Conciseness: [Assessment and explanation]\n",
    "Overall Assessment: [General evaluation summary]\n",
    "GRADE: [CORRECT / PARTIALLY CORRECT / INCORRECT]\n",
    "\n",
    "Now, please evaluate the RAG system's answer based on the provided information. Prefer to use in Korean language.\n",
    "\n",
    "QUESTION: {query}\n",
    "CONTEXT: {context}\n",
    "SYSTEM ANSWER: {result}\n",
    "EVALUATION:\n",
    "\"\"\"\n",
    "\n",
    "custom_qa_prompt = PromptTemplate.from_template(CUSTOM_COT_QA_PROMPT)\n",
    "\n",
    "custom_cot_qa_evaluator = load_evaluator(\n",
    "    evaluator=\"cot_qa\",  \n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0), # OpenAI LLM 사용\n",
    "    prompt=custom_qa_prompt                               # 사용자 지정 프롬프트 사용\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "custom_cot_qa_eval_result = custom_cot_qa_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=context,            # 평가 기준: 문맥\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "custom_cot_qa_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0fe1f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: R1의 좌석 구성은 어떻게 되나요?  \n",
      "CONTEXT: ['.\\n\\n리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페로 피터 스티븐스가 디자인했습니다. 이 차는 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 하며, 2013년 말에서 2014년 초 사이에 생산이 예상되었습니다\\n\\n(참고: 이 문서는 리비안에 대한 정보를 담고 있습니다.)']  \n",
      "SYSTEM ANSWER: R1의 좌석 구성은 2+2 좌석입니다.  \n",
      "\n",
      "EVALUATION:  \n",
      "1. Factual Accuracy:  \n",
      "   - 평가: 시스템의 답변은 \"R1의 좌석 구성은 2+2 좌석입니다.\"라고 명확하게 언급하고 있으며, 이는 제공된 문맥에서 \"2+2 좌석의 미드 엔진 하이브리드 쿠페\"라는 정보와 일치합니다.  \n",
      "   - 설명: 따라서, 사실적으로 정확합니다.\n",
      "\n",
      "2. Relevance:  \n",
      "   - 평가: 질문은 R1의 좌석 구성에 대한 것이며, 시스템의 답변은 이 질문에 직접적으로 답하고 있습니다.  \n",
      "   - 설명: 답변은 질문의 핵심을 잘 반영하고 있어 관련성이 높습니다.\n",
      "\n",
      "3. Completeness:  \n",
      "   - 평가: 시스템의 답변은 R1의 좌석 구성에 대한 질문에 대해 필요한 정보를 제공하고 있습니다.  \n",
      "   - 설명: 그러나, \"2+2 좌석\"이라는 정보 외에 추가적인 설명이나 맥락이 없으므로, 완전성 측면에서 약간의 부족함이 있습니다. 예를 들어, 좌석 구성의 의미나 장점에 대한 설명이 포함되면 더 좋았을 것입니다.\n",
      "\n",
      "4. Conciseness:  \n",
      "   - 평가: 시스템의 답변은 간결하고 핵심 정보를 잘 전달하고 있습니다.  \n",
      "   - 설명: 불필요한 세부사항 없이 질문에 대한 직접적인 답변을 제공하고 있어 간결성 측면에서 우수합니다.\n",
      "\n",
      "Overall Assessment:  \n",
      "시스템의 답변은 사실적으로 정확하고 관련성이 높으며, 간결하게 질문에 답변하고 있습니다. 그러나 완전성 측면에서 약간의 부족함이 있어, 추가적인 설명이 포함되면 더 좋았을 것입니다. 따라서, 이 답변은 일부 기준을 충족하지만 완전성에서 약간의 부족함이 있어 PARTIALLY CORRECT로 평가합니다.\n",
      "\n",
      "GRADE: PARTIALLY CORRECT\n"
     ]
    }
   ],
   "source": [
    "print(custom_cot_qa_eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789d082",
   "metadata": {},
   "source": [
    "`(2) Criteria Evaluation (No lables) - 참조 레이블(ground truth)이 없는 상황에서 모델 출력의 품질을 평가`\n",
    "\n",
    "1. \"criteria\" 평가기:\n",
    "   - 목적: 주어진 기준에 따라 예측이 기준을 만족하는지 평가\n",
    "   - 출력: 이진 점수 (예: Yes/No 또는 1/0)\n",
    "\n",
    "2. \"score_string\" 평가기:\n",
    "   - 목적: 주어진 기준에 따라 예측의 품질을 수치로 평가\n",
    "   - 출력: 수치 점수 (기본적으로 1-10 척도)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c87c4b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'To assess whether the submission meets the criterion of conciseness, I will evaluate the submission step by step.\\n\\n1. **Understanding the Input**: The input question asks about the seating configuration of R1. This indicates that the respondent needs to provide a clear and direct answer regarding the arrangement of seats.\\n\\n2. **Analyzing the Submission**: The submission states, \"R1의 좌석 구성은 2+2 좌석입니다.\" This translates to \"The seating configuration of R1 is 2+2 seats.\" \\n\\n3. **Evaluating Conciseness**: \\n   - The submission directly answers the question posed in the input.\\n   - It uses a straightforward sentence structure without unnecessary words or elaboration.\\n   - The phrase \"2+2 좌석\" succinctly describes the seating arrangement without additional context or filler.\\n\\n4. **Conclusion on Conciseness**: The submission is clear, direct, and does not include any extraneous information. It effectively communicates the necessary information in a brief manner.\\n\\nBased on this analysis, the submission meets the criterion of conciseness.\\n\\nY',\n",
       " 'value': 'Y',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# 간결성 평가 - criteria 평가자 사용\n",
    "conciseness_evaluator = load_evaluator(\n",
    "    evaluator=\"criteria\", \n",
    "    criteria=\"conciseness\",\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "conciseness_result = conciseness_evaluator.evaluate_strings(\n",
    "    input=question,                   # 질문 \n",
    "    prediction=groq_prediction,       # 평가 대상: LLM 모델의 예측\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "conciseness_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d9bb4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To assess whether the submission meets the criterion of conciseness, I will evaluate the submission step by step.\n",
      "\n",
      "1. **Understanding the Input**: The input question asks about the seating configuration of R1. This indicates that the respondent needs to provide a clear and direct answer regarding the arrangement of seats.\n",
      "\n",
      "2. **Analyzing the Submission**: The submission states, \"R1의 좌석 구성은 2+2 좌석입니다.\" This translates to \"The seating configuration of R1 is 2+2 seats.\" \n",
      "\n",
      "3. **Evaluating Conciseness**: \n",
      "   - The submission directly answers the question posed in the input.\n",
      "   - It uses a straightforward sentence structure without unnecessary words or elaboration.\n",
      "   - The phrase \"2+2 좌석\" succinctly describes the seating arrangement without additional context or filler.\n",
      "\n",
      "4. **Conclusion on Conciseness**: The submission is clear, direct, and does not include any extraneous information. It effectively communicates the necessary information in a brief manner.\n",
      "\n",
      "Based on this analysis, the submission meets the criterion of conciseness.\n",
      "\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "print(conciseness_result['reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d2dff0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'To assess the submission based on the provided criteria, I will evaluate each criterion step by step.\\n\\n1. **Relevance**: \\n   - The question asks about the seating configuration of R1. \\n   - The submission states that the seating configuration is \"2+2 좌석\" (2+2 seats).\\n   - This directly answers the question regarding the seating arrangement of R1.\\n   - Therefore, the submission is relevant to the question asked.\\n\\n2. **Conciseness**: \\n   - The submission provides a straightforward answer without any unnecessary details.\\n   - It conveys the key information (the seating configuration) in a clear and succinct manner.\\n   - There are no extraneous words or explanations that detract from the main point.\\n   - Thus, the submission is concise.\\n\\nSince the submission meets both the relevance and conciseness criteria, I conclude that it fulfills the requirements.\\n\\nY',\n",
       " 'value': 'Y',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criteria 직접 지정\n",
    "criteria_evaluator = load_evaluator(\n",
    "    evaluator=\"criteria\", \n",
    "    criteria={\n",
    "        \"relevance\": \"Does the answer appropriately address the question?\",\n",
    "        \"conciseness\": \"Does the answer convey the key information without unnecessary details?\",\n",
    "        },\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "criteria_result = criteria_evaluator.evaluate_strings(\n",
    "    input=question,              # 질문 \n",
    "    prediction=groq_prediction,      # 평가 대상: LLM 모델의 예측\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "criteria_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aaac3ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To assess the submission based on the provided criteria, I will evaluate each criterion step by step.\n",
      "\n",
      "1. **Relevance**: \n",
      "   - The question asks about the seating configuration of R1. \n",
      "   - The submission states that the seating configuration is \"2+2 좌석\" (2+2 seats).\n",
      "   - This directly answers the question regarding the seating arrangement of R1.\n",
      "   - Therefore, the submission is relevant to the question asked.\n",
      "\n",
      "2. **Conciseness**: \n",
      "   - The submission provides a straightforward answer without any unnecessary details.\n",
      "   - It conveys the key information (the seating configuration) in a clear and succinct manner.\n",
      "   - There are no extraneous words or explanations that detract from the main point.\n",
      "   - Thus, the submission is concise.\n",
      "\n",
      "Since the submission meets both the relevance and conciseness criteria, I conclude that it fulfills the requirements.\n",
      "\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "print(criteria_result['reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41934c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.evaluation.scoring.eval_chain:This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'The response provided by the AI assistant directly addresses the user\\'s question about the seating configuration of R1. It specifies that the seating arrangement is \"2+2,\" which is relevant and informative. The answer is concise and provides the necessary information without any extraneous details. Therefore, the relevance of the answer is high.\\n\\nRating: [[10]]',\n",
       " 'score': 1.0}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score_string 평가자 사용\n",
    "score_string_evaluator = load_evaluator(\n",
    "    evaluator=\"score_string\", \n",
    "    criteria={\n",
    "        \"relevance\": \"How relevant is the answer to the question on a scale of 1-10?\",\n",
    "    },\n",
    "    normalize_by=10,\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "score_string_result = score_string_evaluator.evaluate_strings(\n",
    "    input=question,                     # 질문 \n",
    "    prediction=groq_prediction,         # 평가 대상: LLM 모델의 예측\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "score_string_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "744e6f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response provided by the AI assistant directly addresses the user's question about the seating configuration of R1. It specifies that the seating arrangement is \"2+2,\" which is relevant and informative. The answer is concise and provides the necessary information without any extraneous details. Therefore, the relevance of the answer is high.\n",
      "\n",
      "Rating: [[10]]\n"
     ]
    }
   ],
   "source": [
    "print(score_string_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4d4d6",
   "metadata": {},
   "source": [
    "`(3) Criteria Evaluation (With lables) - 참조 레이블(ground truth)이 주어진 상황에서 모델 출력의 품질을 평가`\n",
    "\n",
    "1. \"labeled_criteria\" 평가기:\n",
    "   - 목적: 참조 레이블을 고려하여 예측이 주어진 기준을 만족하는지 평가\n",
    "   - 출력: 이진 점수 (예: Yes/No 또는 1/0)\n",
    "\n",
    "2. \"labeled_score_string\" 평가기:\n",
    "   - 목적: 참조 레이블과 비교하여 예측의 품질을 수치로 평가\n",
    "   - 출력: 수치 점수 (기본적으로 1-10 척도)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76743974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'To assess whether the submission meets the criteria, I will evaluate the correctness of the submission step by step.\\n\\n1. **Understanding the Input**: The input question asks about the seating configuration of R1. This indicates that the expected response should provide factual information regarding how the seats are arranged in R1.\\n\\n2. **Analyzing the Submission**: The submission states, \"R1의 좌석 구성은 2+2 좌석입니다.\" This translates to \"The seating configuration of R1 is 2+2 seats.\" \\n\\n3. **Checking Against the Reference**: The reference states, \"R1은 2+2 좌석 구성입니다.\" This translates to \"R1 has a 2+2 seating configuration.\" \\n\\n4. **Comparing Submission and Reference**: The submission and the reference both convey the same information regarding the seating configuration of R1. The wording is slightly different, but the essential fact remains unchanged.\\n\\n5. **Evaluating Correctness**: Since the submission accurately reflects the information provided in the reference, it is correct, accurate, and factual.\\n\\nBased on this reasoning, the submission meets the criteria for correctness.\\n\\nY',\n",
       " 'value': 'Y',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# labeled_criteria 평가자 사용\n",
    "labeled_crieria_evaluator = load_evaluator(\n",
    "    evaluator=\"labeled_criteria\", \n",
    "    criteria=\"correctness\",\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "labeled_crieria_eval_result = labeled_crieria_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=ground_truth,       # 평가 기준: 정답\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "labeled_crieria_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "97a5e426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To assess whether the submission meets the criteria, I will evaluate the correctness of the submission step by step.\n",
      "\n",
      "1. **Understanding the Input**: The input question asks about the seating configuration of R1. This indicates that the expected response should provide factual information regarding how the seats are arranged in R1.\n",
      "\n",
      "2. **Analyzing the Submission**: The submission states, \"R1의 좌석 구성은 2+2 좌석입니다.\" This translates to \"The seating configuration of R1 is 2+2 seats.\" \n",
      "\n",
      "3. **Checking Against the Reference**: The reference states, \"R1은 2+2 좌석 구성입니다.\" This translates to \"R1 has a 2+2 seating configuration.\" \n",
      "\n",
      "4. **Comparing Submission and Reference**: The submission and the reference both convey the same information regarding the seating configuration of R1. The wording is slightly different, but the essential fact remains unchanged.\n",
      "\n",
      "5. **Evaluating Correctness**: Since the submission accurately reflects the information provided in the reference, it is correct, accurate, and factual.\n",
      "\n",
      "Based on this reasoning, the submission meets the criteria for correctness.\n",
      "\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "print(labeled_crieria_eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ddcd0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'The AI assistant\\'s response accurately states that the seating configuration for R1 is \"2+2,\" which aligns with the provided ground truth. The answer is correct, clear, and directly addresses the user\\'s question about the seating arrangement. There are no inaccuracies or omissions in the response.\\n\\nRating: [[10]]',\n",
       " 'score': 1.0}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labeled_score_string 평가자 사용\n",
    "labeled_score_string_evaluator = load_evaluator(\n",
    "    evaluator=\"labeled_score_string\", \n",
    "    criteria=\"correctness\",\n",
    "    normalize_by=10,\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "labeled_score_string_eval_result = labeled_score_string_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=ground_truth,       # 평가 기준: 정답\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "labeled_score_string_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d0ab755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AI assistant's response accurately states that the seating configuration for R1 is \"2+2,\" which aligns with the provided ground truth. The answer is correct, clear, and directly addresses the user's question about the seating arrangement. There are no inaccuracies or omissions in the response.\n",
      "\n",
      "Rating: [[10]]\n"
     ]
    }
   ],
   "source": [
    "print(labeled_score_string_eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68d831d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness Score: 1\n",
      "Correctness Reasoniong: 설명: 실제 응답 \"R1의 좌석 구성은 2+2 좌석입니다.\"는 기대된 답변 \"R1은 2+2 좌석 구성입니다.\"와 비교했을 때, 사실적으로 정확하고 완전합니다. 두 응답 모두 R1의 좌석 구성이 2+2임을 명확히 전달하고 있습니다. 따라서, 실제 응답은 기대된 답변과 일치합니다.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트를 직접 작성하여 평가 수행 (한국어로 평가)\n",
    "\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "correctness_eval_template = \"\"\"Respond Y or N based on how accurate the given response is compared to the expected answer. Grade only based on the rubric and expected answer:\n",
    "\n",
    "Grading Rubric: {criteria}\n",
    "Question: {input}\n",
    "Expected Answer: {reference}\n",
    "\n",
    "DATA:\n",
    "---------\n",
    "Actual Response: {output}\n",
    "---------\n",
    "Write out your explanation for how accurate the response is compared to the expected answer, considering factual correctness and completeness. Then respond with Y or N on a new line. (in Korean)\"\"\"\n",
    "\n",
    "correctness_eval_prompt = PromptTemplate.from_template(correctness_eval_template)\n",
    "\n",
    "# Correctness: 생성된 답변이 ground truth와 비교하여 정확하고 사실적인지 평가\n",
    "correctness_evaluator = load_evaluator(\n",
    "    evaluator=\"labeled_criteria\", \n",
    "    criteria=\"correctness\",  \n",
    "    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0.0), \n",
    "    prompt=correctness_eval_prompt # 사용자 지정 프롬프트 사용\n",
    "    )\n",
    "\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "correctness_eval_result = correctness_evaluator.evaluate_strings(\n",
    "    input=question,\n",
    "    prediction=groq_prediction,\n",
    "    reference=ground_truth,\n",
    ")\n",
    "\n",
    "print(f'Correctness Score: {correctness_eval_result[\"score\"]}')\n",
    "print(f'Correctness Reasoniong: {correctness_eval_result[\"reasoning\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "286c9181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': '설명: 실제 응답 \"R1의 좌석 구성은 2+2 좌석입니다.\"는 기대된 답변 \"R1은 2+2 좌석 구성입니다.\"와 비교했을 때, 사실적으로 정확하고 완전합니다. 두 응답 모두 R1의 좌석 구성이 2+2임을 명확히 전달하고 있습니다. 따라서, 실제 응답은 기대된 답변과 일치합니다.',\n",
       " 'value': 'Y',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7d5d6461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness Score: 1.0\n",
      "Correctness Reasoning: AI의 답변은 질문에 정확하게 답하고 있으며, 주어진 사실과 일치합니다. R1의 좌석 구성이 2+2라는 정보는 정확하게 전달되었습니다. \n",
      "\n",
      "Rating: [[5]]\n"
     ]
    }
   ],
   "source": [
    "# labled_score_string 평가\n",
    "correctness_score_template = \"\"\"\n",
    "[Instruction]\n",
    "Please act as an impartial judge and evaluate the correctness of the AI assistant's response compared to the ground truth. \n",
    "{criteria}\n",
    "\n",
    "[Ground Truth]\n",
    "{reference}\n",
    "\n",
    "Begin your evaluation by providing a short explanation. Be as objective as possible. Answer in Korean. After providing your explanation, you must rate the response on a scale of 1 to 5 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[3]]\".\n",
    "\n",
    "[Question]\n",
    "{input}\n",
    "\n",
    "[The Start of Assistant's Answer]\n",
    "{prediction}\n",
    "[The End of Assistant's Answer]\n",
    "\"\"\"\n",
    "\n",
    "correctness_score_prompt = PromptTemplate.from_template(correctness_score_template)\n",
    "\n",
    "correctness_criteria = {\n",
    "    \"correctness\": \"\"\"\n",
    "Score 1: The answer is completely incorrect or contradicts the ground truth.\n",
    "Score 2: The answer contains major factual errors or misunderstandings of the ground truth.\n",
    "Score 3: The answer is partially correct but contains some factual errors or omissions.\n",
    "Score 4: The answer is mostly correct with only minor inaccuracies or omissions.\n",
    "Score 5: The answer is completely correct and fully aligned with the ground truth.\"\"\"\n",
    "}\n",
    "\n",
    "correctness_score_evaluator = load_evaluator(\n",
    "    evaluator=\"labeled_score_string\",  \n",
    "    criteria=correctness_criteria,   \n",
    "    normalize_by=5,                \n",
    "    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0.0), \n",
    "    prompt=correctness_score_prompt \n",
    ")\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "correctness_score_result = correctness_score_evaluator.evaluate_strings(\n",
    "    input=question,                   # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,       # 평가 대상: LLM 모델의 예측\n",
    "    reference=ground_truth,           # 평가 기준: 정답\n",
    ")\n",
    "\n",
    "print(f'Correctness Score: {correctness_score_result[\"score\"]}')\n",
    "print(f'Correctness Reasoning: {correctness_score_result[\"reasoning\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
